// Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT.

package shared

import (
	"bytes"
	"encoding/json"
	"errors"
	"fmt"
)

// DestinationBigqueryDatasetLocationEnum - The location of the dataset. Warning: Changes made after creation will not be applied. Read more <a href="https://cloud.google.com/bigquery/docs/locations">here</a>.
type DestinationBigqueryDatasetLocationEnum string

const (
	DestinationBigqueryDatasetLocationEnumUs                     DestinationBigqueryDatasetLocationEnum = "US"
	DestinationBigqueryDatasetLocationEnumEu                     DestinationBigqueryDatasetLocationEnum = "EU"
	DestinationBigqueryDatasetLocationEnumAsiaEast1              DestinationBigqueryDatasetLocationEnum = "asia-east1"
	DestinationBigqueryDatasetLocationEnumAsiaEast2              DestinationBigqueryDatasetLocationEnum = "asia-east2"
	DestinationBigqueryDatasetLocationEnumAsiaNortheast1         DestinationBigqueryDatasetLocationEnum = "asia-northeast1"
	DestinationBigqueryDatasetLocationEnumAsiaNortheast2         DestinationBigqueryDatasetLocationEnum = "asia-northeast2"
	DestinationBigqueryDatasetLocationEnumAsiaNortheast3         DestinationBigqueryDatasetLocationEnum = "asia-northeast3"
	DestinationBigqueryDatasetLocationEnumAsiaSouth1             DestinationBigqueryDatasetLocationEnum = "asia-south1"
	DestinationBigqueryDatasetLocationEnumAsiaSouth2             DestinationBigqueryDatasetLocationEnum = "asia-south2"
	DestinationBigqueryDatasetLocationEnumAsiaSoutheast1         DestinationBigqueryDatasetLocationEnum = "asia-southeast1"
	DestinationBigqueryDatasetLocationEnumAsiaSoutheast2         DestinationBigqueryDatasetLocationEnum = "asia-southeast2"
	DestinationBigqueryDatasetLocationEnumAustraliaSoutheast1    DestinationBigqueryDatasetLocationEnum = "australia-southeast1"
	DestinationBigqueryDatasetLocationEnumAustraliaSoutheast2    DestinationBigqueryDatasetLocationEnum = "australia-southeast2"
	DestinationBigqueryDatasetLocationEnumEuropeCentral2         DestinationBigqueryDatasetLocationEnum = "europe-central2"
	DestinationBigqueryDatasetLocationEnumEuropeNorth1           DestinationBigqueryDatasetLocationEnum = "europe-north1"
	DestinationBigqueryDatasetLocationEnumEuropeWest1            DestinationBigqueryDatasetLocationEnum = "europe-west1"
	DestinationBigqueryDatasetLocationEnumEuropeWest2            DestinationBigqueryDatasetLocationEnum = "europe-west2"
	DestinationBigqueryDatasetLocationEnumEuropeWest3            DestinationBigqueryDatasetLocationEnum = "europe-west3"
	DestinationBigqueryDatasetLocationEnumEuropeWest4            DestinationBigqueryDatasetLocationEnum = "europe-west4"
	DestinationBigqueryDatasetLocationEnumEuropeWest6            DestinationBigqueryDatasetLocationEnum = "europe-west6"
	DestinationBigqueryDatasetLocationEnumNorthamericaNortheast1 DestinationBigqueryDatasetLocationEnum = "northamerica-northeast1"
	DestinationBigqueryDatasetLocationEnumNorthamericaNortheast2 DestinationBigqueryDatasetLocationEnum = "northamerica-northeast2"
	DestinationBigqueryDatasetLocationEnumSouthamericaEast1      DestinationBigqueryDatasetLocationEnum = "southamerica-east1"
	DestinationBigqueryDatasetLocationEnumSouthamericaWest1      DestinationBigqueryDatasetLocationEnum = "southamerica-west1"
	DestinationBigqueryDatasetLocationEnumUsCentral1             DestinationBigqueryDatasetLocationEnum = "us-central1"
	DestinationBigqueryDatasetLocationEnumUsEast1                DestinationBigqueryDatasetLocationEnum = "us-east1"
	DestinationBigqueryDatasetLocationEnumUsEast4                DestinationBigqueryDatasetLocationEnum = "us-east4"
	DestinationBigqueryDatasetLocationEnumUsWest1                DestinationBigqueryDatasetLocationEnum = "us-west1"
	DestinationBigqueryDatasetLocationEnumUsWest2                DestinationBigqueryDatasetLocationEnum = "us-west2"
	DestinationBigqueryDatasetLocationEnumUsWest3                DestinationBigqueryDatasetLocationEnum = "us-west3"
	DestinationBigqueryDatasetLocationEnumUsWest4                DestinationBigqueryDatasetLocationEnum = "us-west4"
)

func (e *DestinationBigqueryDatasetLocationEnum) UnmarshalJSON(data []byte) error {
	var s string
	if err := json.Unmarshal(data, &s); err != nil {
		return err
	}
	switch s {
	case "US":
		fallthrough
	case "EU":
		fallthrough
	case "asia-east1":
		fallthrough
	case "asia-east2":
		fallthrough
	case "asia-northeast1":
		fallthrough
	case "asia-northeast2":
		fallthrough
	case "asia-northeast3":
		fallthrough
	case "asia-south1":
		fallthrough
	case "asia-south2":
		fallthrough
	case "asia-southeast1":
		fallthrough
	case "asia-southeast2":
		fallthrough
	case "australia-southeast1":
		fallthrough
	case "australia-southeast2":
		fallthrough
	case "europe-central2":
		fallthrough
	case "europe-north1":
		fallthrough
	case "europe-west1":
		fallthrough
	case "europe-west2":
		fallthrough
	case "europe-west3":
		fallthrough
	case "europe-west4":
		fallthrough
	case "europe-west6":
		fallthrough
	case "northamerica-northeast1":
		fallthrough
	case "northamerica-northeast2":
		fallthrough
	case "southamerica-east1":
		fallthrough
	case "southamerica-west1":
		fallthrough
	case "us-central1":
		fallthrough
	case "us-east1":
		fallthrough
	case "us-east4":
		fallthrough
	case "us-west1":
		fallthrough
	case "us-west2":
		fallthrough
	case "us-west3":
		fallthrough
	case "us-west4":
		*e = DestinationBigqueryDatasetLocationEnum(s)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationBigqueryDatasetLocationEnum: %s", s)
	}
}

type DestinationBigqueryBigqueryEnum string

const (
	DestinationBigqueryBigqueryEnumBigquery DestinationBigqueryBigqueryEnum = "bigquery"
)

func (e *DestinationBigqueryBigqueryEnum) UnmarshalJSON(data []byte) error {
	var s string
	if err := json.Unmarshal(data, &s); err != nil {
		return err
	}
	switch s {
	case "bigquery":
		*e = DestinationBigqueryBigqueryEnum(s)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationBigqueryBigqueryEnum: %s", s)
	}
}

type DestinationBigqueryLoadingMethodGCSStagingCredentialHMACKeyCredentialTypeEnum string

const (
	DestinationBigqueryLoadingMethodGCSStagingCredentialHMACKeyCredentialTypeEnumHmacKey DestinationBigqueryLoadingMethodGCSStagingCredentialHMACKeyCredentialTypeEnum = "HMAC_KEY"
)

func (e *DestinationBigqueryLoadingMethodGCSStagingCredentialHMACKeyCredentialTypeEnum) UnmarshalJSON(data []byte) error {
	var s string
	if err := json.Unmarshal(data, &s); err != nil {
		return err
	}
	switch s {
	case "HMAC_KEY":
		*e = DestinationBigqueryLoadingMethodGCSStagingCredentialHMACKeyCredentialTypeEnum(s)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationBigqueryLoadingMethodGCSStagingCredentialHMACKeyCredentialTypeEnum: %s", s)
	}
}

// DestinationBigqueryLoadingMethodGCSStagingCredentialHMACKey - An HMAC key is a type of credential and can be associated with a service account or a user account in Cloud Storage. Read more <a href="https://cloud.google.com/storage/docs/authentication/hmackeys">here</a>.
type DestinationBigqueryLoadingMethodGCSStagingCredentialHMACKey struct {
	CredentialType DestinationBigqueryLoadingMethodGCSStagingCredentialHMACKeyCredentialTypeEnum `json:"credential_type"`
	// HMAC key access ID. When linked to a service account, this ID is 61 characters long; when linked to a user account, it is 24 characters long.
	HmacKeyAccessID string `json:"hmac_key_access_id"`
	// The corresponding secret for the access ID. It is a 40-character base-64 encoded string.
	HmacKeySecret string `json:"hmac_key_secret"`
}

type DestinationBigqueryLoadingMethodGCSStagingCredentialType string

const (
	DestinationBigqueryLoadingMethodGCSStagingCredentialTypeDestinationBigqueryLoadingMethodGCSStagingCredentialHMACKey DestinationBigqueryLoadingMethodGCSStagingCredentialType = "destination-bigquery_Loading Method_GCS Staging_Credential_HMAC key"
)

type DestinationBigqueryLoadingMethodGCSStagingCredential struct {
	DestinationBigqueryLoadingMethodGCSStagingCredentialHMACKey *DestinationBigqueryLoadingMethodGCSStagingCredentialHMACKey

	Type DestinationBigqueryLoadingMethodGCSStagingCredentialType
}

func CreateDestinationBigqueryLoadingMethodGCSStagingCredentialDestinationBigqueryLoadingMethodGCSStagingCredentialHMACKey(destinationBigqueryLoadingMethodGCSStagingCredentialHMACKey DestinationBigqueryLoadingMethodGCSStagingCredentialHMACKey) DestinationBigqueryLoadingMethodGCSStagingCredential {
	typ := DestinationBigqueryLoadingMethodGCSStagingCredentialTypeDestinationBigqueryLoadingMethodGCSStagingCredentialHMACKey

	return DestinationBigqueryLoadingMethodGCSStagingCredential{
		DestinationBigqueryLoadingMethodGCSStagingCredentialHMACKey: &destinationBigqueryLoadingMethodGCSStagingCredentialHMACKey,
		Type: typ,
	}
}

func (u *DestinationBigqueryLoadingMethodGCSStagingCredential) UnmarshalJSON(data []byte) error {
	var d *json.Decoder

	destinationBigqueryLoadingMethodGCSStagingCredentialHMACKey := new(DestinationBigqueryLoadingMethodGCSStagingCredentialHMACKey)
	d = json.NewDecoder(bytes.NewReader(data))
	d.DisallowUnknownFields()
	if err := d.Decode(&destinationBigqueryLoadingMethodGCSStagingCredentialHMACKey); err == nil {
		u.DestinationBigqueryLoadingMethodGCSStagingCredentialHMACKey = destinationBigqueryLoadingMethodGCSStagingCredentialHMACKey
		u.Type = DestinationBigqueryLoadingMethodGCSStagingCredentialTypeDestinationBigqueryLoadingMethodGCSStagingCredentialHMACKey
		return nil
	}

	return errors.New("could not unmarshal into supported union types")
}

func (u DestinationBigqueryLoadingMethodGCSStagingCredential) MarshalJSON() ([]byte, error) {
	if u.DestinationBigqueryLoadingMethodGCSStagingCredentialHMACKey != nil {
		return json.Marshal(u.DestinationBigqueryLoadingMethodGCSStagingCredentialHMACKey)
	}

	return nil, nil
}

// DestinationBigqueryLoadingMethodGCSStagingGCSTmpFilesAfterwardProcessingEnum - This upload method is supposed to temporary store records in GCS bucket. By this select you can chose if these records should be removed from GCS when migration has finished. The default "Delete all tmp files from GCS" value is used if not set explicitly.
type DestinationBigqueryLoadingMethodGCSStagingGCSTmpFilesAfterwardProcessingEnum string

const (
	DestinationBigqueryLoadingMethodGCSStagingGCSTmpFilesAfterwardProcessingEnumDeleteAllTmpFilesFromGcs DestinationBigqueryLoadingMethodGCSStagingGCSTmpFilesAfterwardProcessingEnum = "Delete all tmp files from GCS"
	DestinationBigqueryLoadingMethodGCSStagingGCSTmpFilesAfterwardProcessingEnumKeepAllTmpFilesInGcs     DestinationBigqueryLoadingMethodGCSStagingGCSTmpFilesAfterwardProcessingEnum = "Keep all tmp files in GCS"
)

func (e *DestinationBigqueryLoadingMethodGCSStagingGCSTmpFilesAfterwardProcessingEnum) UnmarshalJSON(data []byte) error {
	var s string
	if err := json.Unmarshal(data, &s); err != nil {
		return err
	}
	switch s {
	case "Delete all tmp files from GCS":
		fallthrough
	case "Keep all tmp files in GCS":
		*e = DestinationBigqueryLoadingMethodGCSStagingGCSTmpFilesAfterwardProcessingEnum(s)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationBigqueryLoadingMethodGCSStagingGCSTmpFilesAfterwardProcessingEnum: %s", s)
	}
}

type DestinationBigqueryLoadingMethodGCSStagingMethodEnum string

const (
	DestinationBigqueryLoadingMethodGCSStagingMethodEnumGcsStaging DestinationBigqueryLoadingMethodGCSStagingMethodEnum = "GCS Staging"
)

func (e *DestinationBigqueryLoadingMethodGCSStagingMethodEnum) UnmarshalJSON(data []byte) error {
	var s string
	if err := json.Unmarshal(data, &s); err != nil {
		return err
	}
	switch s {
	case "GCS Staging":
		*e = DestinationBigqueryLoadingMethodGCSStagingMethodEnum(s)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationBigqueryLoadingMethodGCSStagingMethodEnum: %s", s)
	}
}

// DestinationBigqueryLoadingMethodGCSStaging - Loading method used to send select the way data will be uploaded to BigQuery. <br/><b>Standard Inserts</b> - Direct uploading using SQL INSERT statements. This method is extremely inefficient and provided only for quick testing. In almost all cases, you should use staging. <br/><b>GCS Staging</b> - Writes large batches of records to a file, uploads the file to GCS, then uses <b>COPY INTO table</b> to upload the file. Recommended for most workloads for better speed and scalability. Read more about GCS Staging <a href="https://docs.airbyte.com/integrations/destinations/bigquery#gcs-staging">here</a>.
type DestinationBigqueryLoadingMethodGCSStaging struct {
	// An HMAC key is a type of credential and can be associated with a service account or a user account in Cloud Storage. Read more <a href="https://cloud.google.com/storage/docs/authentication/hmackeys">here</a>.
	Credential DestinationBigqueryLoadingMethodGCSStagingCredential `json:"credential"`
	// The name of the GCS bucket. Read more <a href="https://cloud.google.com/storage/docs/naming-buckets">here</a>.
	GcsBucketName string `json:"gcs_bucket_name"`
	// Directory under the GCS bucket where data will be written.
	GcsBucketPath string `json:"gcs_bucket_path"`
	// This upload method is supposed to temporary store records in GCS bucket. By this select you can chose if these records should be removed from GCS when migration has finished. The default "Delete all tmp files from GCS" value is used if not set explicitly.
	KeepFilesInGcsBucket *DestinationBigqueryLoadingMethodGCSStagingGCSTmpFilesAfterwardProcessingEnum `json:"keep_files_in_gcs-bucket,omitempty"`
	Method               DestinationBigqueryLoadingMethodGCSStagingMethodEnum                          `json:"method"`
}

type DestinationBigqueryLoadingMethodStandardInsertsMethodEnum string

const (
	DestinationBigqueryLoadingMethodStandardInsertsMethodEnumStandard DestinationBigqueryLoadingMethodStandardInsertsMethodEnum = "Standard"
)

func (e *DestinationBigqueryLoadingMethodStandardInsertsMethodEnum) UnmarshalJSON(data []byte) error {
	var s string
	if err := json.Unmarshal(data, &s); err != nil {
		return err
	}
	switch s {
	case "Standard":
		*e = DestinationBigqueryLoadingMethodStandardInsertsMethodEnum(s)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationBigqueryLoadingMethodStandardInsertsMethodEnum: %s", s)
	}
}

// DestinationBigqueryLoadingMethodStandardInserts - Loading method used to send select the way data will be uploaded to BigQuery. <br/><b>Standard Inserts</b> - Direct uploading using SQL INSERT statements. This method is extremely inefficient and provided only for quick testing. In almost all cases, you should use staging. <br/><b>GCS Staging</b> - Writes large batches of records to a file, uploads the file to GCS, then uses <b>COPY INTO table</b> to upload the file. Recommended for most workloads for better speed and scalability. Read more about GCS Staging <a href="https://docs.airbyte.com/integrations/destinations/bigquery#gcs-staging">here</a>.
type DestinationBigqueryLoadingMethodStandardInserts struct {
	Method DestinationBigqueryLoadingMethodStandardInsertsMethodEnum `json:"method"`
}

type DestinationBigqueryLoadingMethodType string

const (
	DestinationBigqueryLoadingMethodTypeDestinationBigqueryLoadingMethodStandardInserts DestinationBigqueryLoadingMethodType = "destination-bigquery_Loading Method_Standard Inserts"
	DestinationBigqueryLoadingMethodTypeDestinationBigqueryLoadingMethodGCSStaging      DestinationBigqueryLoadingMethodType = "destination-bigquery_Loading Method_GCS Staging"
)

type DestinationBigqueryLoadingMethod struct {
	DestinationBigqueryLoadingMethodStandardInserts *DestinationBigqueryLoadingMethodStandardInserts
	DestinationBigqueryLoadingMethodGCSStaging      *DestinationBigqueryLoadingMethodGCSStaging

	Type DestinationBigqueryLoadingMethodType
}

func CreateDestinationBigqueryLoadingMethodDestinationBigqueryLoadingMethodStandardInserts(destinationBigqueryLoadingMethodStandardInserts DestinationBigqueryLoadingMethodStandardInserts) DestinationBigqueryLoadingMethod {
	typ := DestinationBigqueryLoadingMethodTypeDestinationBigqueryLoadingMethodStandardInserts

	return DestinationBigqueryLoadingMethod{
		DestinationBigqueryLoadingMethodStandardInserts: &destinationBigqueryLoadingMethodStandardInserts,
		Type: typ,
	}
}

func CreateDestinationBigqueryLoadingMethodDestinationBigqueryLoadingMethodGCSStaging(destinationBigqueryLoadingMethodGCSStaging DestinationBigqueryLoadingMethodGCSStaging) DestinationBigqueryLoadingMethod {
	typ := DestinationBigqueryLoadingMethodTypeDestinationBigqueryLoadingMethodGCSStaging

	return DestinationBigqueryLoadingMethod{
		DestinationBigqueryLoadingMethodGCSStaging: &destinationBigqueryLoadingMethodGCSStaging,
		Type: typ,
	}
}

func (u *DestinationBigqueryLoadingMethod) UnmarshalJSON(data []byte) error {
	var d *json.Decoder

	destinationBigqueryLoadingMethodStandardInserts := new(DestinationBigqueryLoadingMethodStandardInserts)
	d = json.NewDecoder(bytes.NewReader(data))
	d.DisallowUnknownFields()
	if err := d.Decode(&destinationBigqueryLoadingMethodStandardInserts); err == nil {
		u.DestinationBigqueryLoadingMethodStandardInserts = destinationBigqueryLoadingMethodStandardInserts
		u.Type = DestinationBigqueryLoadingMethodTypeDestinationBigqueryLoadingMethodStandardInserts
		return nil
	}

	destinationBigqueryLoadingMethodGCSStaging := new(DestinationBigqueryLoadingMethodGCSStaging)
	d = json.NewDecoder(bytes.NewReader(data))
	d.DisallowUnknownFields()
	if err := d.Decode(&destinationBigqueryLoadingMethodGCSStaging); err == nil {
		u.DestinationBigqueryLoadingMethodGCSStaging = destinationBigqueryLoadingMethodGCSStaging
		u.Type = DestinationBigqueryLoadingMethodTypeDestinationBigqueryLoadingMethodGCSStaging
		return nil
	}

	return errors.New("could not unmarshal into supported union types")
}

func (u DestinationBigqueryLoadingMethod) MarshalJSON() ([]byte, error) {
	if u.DestinationBigqueryLoadingMethodStandardInserts != nil {
		return json.Marshal(u.DestinationBigqueryLoadingMethodStandardInserts)
	}

	if u.DestinationBigqueryLoadingMethodGCSStaging != nil {
		return json.Marshal(u.DestinationBigqueryLoadingMethodGCSStaging)
	}

	return nil, nil
}

// DestinationBigqueryTransformationQueryRunTypeEnum - Interactive run type means that the query is executed as soon as possible, and these queries count towards concurrent rate limit and daily limit. Read more about interactive run type <a href="https://cloud.google.com/bigquery/docs/running-queries#queries">here</a>. Batch queries are queued and started as soon as idle resources are available in the BigQuery shared resource pool, which usually occurs within a few minutes. Batch queries don’t count towards your concurrent rate limit. Read more about batch queries <a href="https://cloud.google.com/bigquery/docs/running-queries#batch">here</a>. The default "interactive" value is used if not set explicitly.
type DestinationBigqueryTransformationQueryRunTypeEnum string

const (
	DestinationBigqueryTransformationQueryRunTypeEnumInteractive DestinationBigqueryTransformationQueryRunTypeEnum = "interactive"
	DestinationBigqueryTransformationQueryRunTypeEnumBatch       DestinationBigqueryTransformationQueryRunTypeEnum = "batch"
)

func (e *DestinationBigqueryTransformationQueryRunTypeEnum) UnmarshalJSON(data []byte) error {
	var s string
	if err := json.Unmarshal(data, &s); err != nil {
		return err
	}
	switch s {
	case "interactive":
		fallthrough
	case "batch":
		*e = DestinationBigqueryTransformationQueryRunTypeEnum(s)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationBigqueryTransformationQueryRunTypeEnum: %s", s)
	}
}

// DestinationBigquery - The values required to configure the destination.
type DestinationBigquery struct {
	// Google BigQuery client's chunk (buffer) size (MIN=1, MAX = 15) for each table. The size that will be written by a single RPC. Written data will be buffered and only flushed upon reaching this size or closing the channel. The default 15MB value is used if not set explicitly. Read more <a href="https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.client.Client.html">here</a>.
	BigQueryClientBufferSizeMb *int64 `json:"big_query_client_buffer_size_mb,omitempty"`
	// The contents of the JSON service account key. Check out the <a href="https://docs.airbyte.com/integrations/destinations/bigquery#service-account-key">docs</a> if you need help generating this key. Default credentials will be used if this field is left empty.
	CredentialsJSON *string `json:"credentials_json,omitempty"`
	// The default BigQuery Dataset ID that tables are replicated to if the source does not specify a namespace. Read more <a href="https://cloud.google.com/bigquery/docs/datasets#create-dataset">here</a>.
	DatasetID string `json:"dataset_id"`
	// The location of the dataset. Warning: Changes made after creation will not be applied. Read more <a href="https://cloud.google.com/bigquery/docs/locations">here</a>.
	DatasetLocation DestinationBigqueryDatasetLocationEnum `json:"dataset_location"`
	DestinationType DestinationBigqueryBigqueryEnum        `json:"destinationType"`
	// Loading method used to send select the way data will be uploaded to BigQuery. <br/><b>Standard Inserts</b> - Direct uploading using SQL INSERT statements. This method is extremely inefficient and provided only for quick testing. In almost all cases, you should use staging. <br/><b>GCS Staging</b> - Writes large batches of records to a file, uploads the file to GCS, then uses <b>COPY INTO table</b> to upload the file. Recommended for most workloads for better speed and scalability. Read more about GCS Staging <a href="https://docs.airbyte.com/integrations/destinations/bigquery#gcs-staging">here</a>.
	LoadingMethod *DestinationBigqueryLoadingMethod `json:"loading_method,omitempty"`
	// The GCP project ID for the project containing the target BigQuery dataset. Read more <a href="https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects">here</a>.
	ProjectID string `json:"project_id"`
	// Interactive run type means that the query is executed as soon as possible, and these queries count towards concurrent rate limit and daily limit. Read more about interactive run type <a href="https://cloud.google.com/bigquery/docs/running-queries#queries">here</a>. Batch queries are queued and started as soon as idle resources are available in the BigQuery shared resource pool, which usually occurs within a few minutes. Batch queries don’t count towards your concurrent rate limit. Read more about batch queries <a href="https://cloud.google.com/bigquery/docs/running-queries#batch">here</a>. The default "interactive" value is used if not set explicitly.
	TransformationPriority *DestinationBigqueryTransformationQueryRunTypeEnum `json:"transformation_priority,omitempty"`
}
