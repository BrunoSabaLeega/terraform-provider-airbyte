// Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT.

package shared

import (
	"bytes"
	"encoding/json"
	"errors"
	"fmt"
)

// DestinationBigqueryDenormalizedDatasetLocationEnum - The location of the dataset. Warning: Changes made after creation will not be applied. The default "US" value is used if not set explicitly. Read more <a href="https://cloud.google.com/bigquery/docs/locations">here</a>.
type DestinationBigqueryDenormalizedDatasetLocationEnum string

const (
	DestinationBigqueryDenormalizedDatasetLocationEnumUs                     DestinationBigqueryDenormalizedDatasetLocationEnum = "US"
	DestinationBigqueryDenormalizedDatasetLocationEnumEu                     DestinationBigqueryDenormalizedDatasetLocationEnum = "EU"
	DestinationBigqueryDenormalizedDatasetLocationEnumAsiaEast1              DestinationBigqueryDenormalizedDatasetLocationEnum = "asia-east1"
	DestinationBigqueryDenormalizedDatasetLocationEnumAsiaEast2              DestinationBigqueryDenormalizedDatasetLocationEnum = "asia-east2"
	DestinationBigqueryDenormalizedDatasetLocationEnumAsiaNortheast1         DestinationBigqueryDenormalizedDatasetLocationEnum = "asia-northeast1"
	DestinationBigqueryDenormalizedDatasetLocationEnumAsiaNortheast2         DestinationBigqueryDenormalizedDatasetLocationEnum = "asia-northeast2"
	DestinationBigqueryDenormalizedDatasetLocationEnumAsiaNortheast3         DestinationBigqueryDenormalizedDatasetLocationEnum = "asia-northeast3"
	DestinationBigqueryDenormalizedDatasetLocationEnumAsiaSouth1             DestinationBigqueryDenormalizedDatasetLocationEnum = "asia-south1"
	DestinationBigqueryDenormalizedDatasetLocationEnumAsiaSouth2             DestinationBigqueryDenormalizedDatasetLocationEnum = "asia-south2"
	DestinationBigqueryDenormalizedDatasetLocationEnumAsiaSoutheast1         DestinationBigqueryDenormalizedDatasetLocationEnum = "asia-southeast1"
	DestinationBigqueryDenormalizedDatasetLocationEnumAsiaSoutheast2         DestinationBigqueryDenormalizedDatasetLocationEnum = "asia-southeast2"
	DestinationBigqueryDenormalizedDatasetLocationEnumAustraliaSoutheast1    DestinationBigqueryDenormalizedDatasetLocationEnum = "australia-southeast1"
	DestinationBigqueryDenormalizedDatasetLocationEnumAustraliaSoutheast2    DestinationBigqueryDenormalizedDatasetLocationEnum = "australia-southeast2"
	DestinationBigqueryDenormalizedDatasetLocationEnumEuropeCentral1         DestinationBigqueryDenormalizedDatasetLocationEnum = "europe-central1"
	DestinationBigqueryDenormalizedDatasetLocationEnumEuropeCentral2         DestinationBigqueryDenormalizedDatasetLocationEnum = "europe-central2"
	DestinationBigqueryDenormalizedDatasetLocationEnumEuropeNorth1           DestinationBigqueryDenormalizedDatasetLocationEnum = "europe-north1"
	DestinationBigqueryDenormalizedDatasetLocationEnumEuropeSouthwest1       DestinationBigqueryDenormalizedDatasetLocationEnum = "europe-southwest1"
	DestinationBigqueryDenormalizedDatasetLocationEnumEuropeWest1            DestinationBigqueryDenormalizedDatasetLocationEnum = "europe-west1"
	DestinationBigqueryDenormalizedDatasetLocationEnumEuropeWest2            DestinationBigqueryDenormalizedDatasetLocationEnum = "europe-west2"
	DestinationBigqueryDenormalizedDatasetLocationEnumEuropeWest3            DestinationBigqueryDenormalizedDatasetLocationEnum = "europe-west3"
	DestinationBigqueryDenormalizedDatasetLocationEnumEuropeWest4            DestinationBigqueryDenormalizedDatasetLocationEnum = "europe-west4"
	DestinationBigqueryDenormalizedDatasetLocationEnumEuropeWest6            DestinationBigqueryDenormalizedDatasetLocationEnum = "europe-west6"
	DestinationBigqueryDenormalizedDatasetLocationEnumEuropeWest7            DestinationBigqueryDenormalizedDatasetLocationEnum = "europe-west7"
	DestinationBigqueryDenormalizedDatasetLocationEnumEuropeWest8            DestinationBigqueryDenormalizedDatasetLocationEnum = "europe-west8"
	DestinationBigqueryDenormalizedDatasetLocationEnumEuropeWest9            DestinationBigqueryDenormalizedDatasetLocationEnum = "europe-west9"
	DestinationBigqueryDenormalizedDatasetLocationEnumMeWest1                DestinationBigqueryDenormalizedDatasetLocationEnum = "me-west1"
	DestinationBigqueryDenormalizedDatasetLocationEnumNorthamericaNortheast1 DestinationBigqueryDenormalizedDatasetLocationEnum = "northamerica-northeast1"
	DestinationBigqueryDenormalizedDatasetLocationEnumNorthamericaNortheast2 DestinationBigqueryDenormalizedDatasetLocationEnum = "northamerica-northeast2"
	DestinationBigqueryDenormalizedDatasetLocationEnumSouthamericaEast1      DestinationBigqueryDenormalizedDatasetLocationEnum = "southamerica-east1"
	DestinationBigqueryDenormalizedDatasetLocationEnumSouthamericaWest1      DestinationBigqueryDenormalizedDatasetLocationEnum = "southamerica-west1"
	DestinationBigqueryDenormalizedDatasetLocationEnumUsCentral1             DestinationBigqueryDenormalizedDatasetLocationEnum = "us-central1"
	DestinationBigqueryDenormalizedDatasetLocationEnumUsEast1                DestinationBigqueryDenormalizedDatasetLocationEnum = "us-east1"
	DestinationBigqueryDenormalizedDatasetLocationEnumUsEast2                DestinationBigqueryDenormalizedDatasetLocationEnum = "us-east2"
	DestinationBigqueryDenormalizedDatasetLocationEnumUsEast3                DestinationBigqueryDenormalizedDatasetLocationEnum = "us-east3"
	DestinationBigqueryDenormalizedDatasetLocationEnumUsEast4                DestinationBigqueryDenormalizedDatasetLocationEnum = "us-east4"
	DestinationBigqueryDenormalizedDatasetLocationEnumUsEast5                DestinationBigqueryDenormalizedDatasetLocationEnum = "us-east5"
	DestinationBigqueryDenormalizedDatasetLocationEnumUsWest1                DestinationBigqueryDenormalizedDatasetLocationEnum = "us-west1"
	DestinationBigqueryDenormalizedDatasetLocationEnumUsWest2                DestinationBigqueryDenormalizedDatasetLocationEnum = "us-west2"
	DestinationBigqueryDenormalizedDatasetLocationEnumUsWest3                DestinationBigqueryDenormalizedDatasetLocationEnum = "us-west3"
	DestinationBigqueryDenormalizedDatasetLocationEnumUsWest4                DestinationBigqueryDenormalizedDatasetLocationEnum = "us-west4"
)

func (e DestinationBigqueryDenormalizedDatasetLocationEnum) ToPointer() *DestinationBigqueryDenormalizedDatasetLocationEnum {
	return &e
}

func (e *DestinationBigqueryDenormalizedDatasetLocationEnum) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "US":
		fallthrough
	case "EU":
		fallthrough
	case "asia-east1":
		fallthrough
	case "asia-east2":
		fallthrough
	case "asia-northeast1":
		fallthrough
	case "asia-northeast2":
		fallthrough
	case "asia-northeast3":
		fallthrough
	case "asia-south1":
		fallthrough
	case "asia-south2":
		fallthrough
	case "asia-southeast1":
		fallthrough
	case "asia-southeast2":
		fallthrough
	case "australia-southeast1":
		fallthrough
	case "australia-southeast2":
		fallthrough
	case "europe-central1":
		fallthrough
	case "europe-central2":
		fallthrough
	case "europe-north1":
		fallthrough
	case "europe-southwest1":
		fallthrough
	case "europe-west1":
		fallthrough
	case "europe-west2":
		fallthrough
	case "europe-west3":
		fallthrough
	case "europe-west4":
		fallthrough
	case "europe-west6":
		fallthrough
	case "europe-west7":
		fallthrough
	case "europe-west8":
		fallthrough
	case "europe-west9":
		fallthrough
	case "me-west1":
		fallthrough
	case "northamerica-northeast1":
		fallthrough
	case "northamerica-northeast2":
		fallthrough
	case "southamerica-east1":
		fallthrough
	case "southamerica-west1":
		fallthrough
	case "us-central1":
		fallthrough
	case "us-east1":
		fallthrough
	case "us-east2":
		fallthrough
	case "us-east3":
		fallthrough
	case "us-east4":
		fallthrough
	case "us-east5":
		fallthrough
	case "us-west1":
		fallthrough
	case "us-west2":
		fallthrough
	case "us-west3":
		fallthrough
	case "us-west4":
		*e = DestinationBigqueryDenormalizedDatasetLocationEnum(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationBigqueryDenormalizedDatasetLocationEnum: %v", v)
	}
}

type DestinationBigqueryDenormalizedBigqueryDenormalizedEnum string

const (
	DestinationBigqueryDenormalizedBigqueryDenormalizedEnumBigqueryDenormalized DestinationBigqueryDenormalizedBigqueryDenormalizedEnum = "bigquery-denormalized"
)

func (e DestinationBigqueryDenormalizedBigqueryDenormalizedEnum) ToPointer() *DestinationBigqueryDenormalizedBigqueryDenormalizedEnum {
	return &e
}

func (e *DestinationBigqueryDenormalizedBigqueryDenormalizedEnum) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "bigquery-denormalized":
		*e = DestinationBigqueryDenormalizedBigqueryDenormalizedEnum(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationBigqueryDenormalizedBigqueryDenormalizedEnum: %v", v)
	}
}

type DestinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialHMACKeyCredentialTypeEnum string

const (
	DestinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialHMACKeyCredentialTypeEnumHmacKey DestinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialHMACKeyCredentialTypeEnum = "HMAC_KEY"
)

func (e DestinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialHMACKeyCredentialTypeEnum) ToPointer() *DestinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialHMACKeyCredentialTypeEnum {
	return &e
}

func (e *DestinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialHMACKeyCredentialTypeEnum) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "HMAC_KEY":
		*e = DestinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialHMACKeyCredentialTypeEnum(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialHMACKeyCredentialTypeEnum: %v", v)
	}
}

// DestinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialHMACKey - An HMAC key is a type of credential and can be associated with a service account or a user account in Cloud Storage. Read more <a href="https://cloud.google.com/storage/docs/authentication/hmackeys">here</a>.
type DestinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialHMACKey struct {
	CredentialType DestinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialHMACKeyCredentialTypeEnum `json:"credential_type"`
	// HMAC key access ID. When linked to a service account, this ID is 61 characters long; when linked to a user account, it is 24 characters long.
	HmacKeyAccessID string `json:"hmac_key_access_id"`
	// The corresponding secret for the access ID. It is a 40-character base-64 encoded string.
	HmacKeySecret string `json:"hmac_key_secret"`
}

type DestinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialType string

const (
	DestinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialTypeDestinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialHMACKey DestinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialType = "destination-bigquery-denormalized_Loading Method_GCS Staging_Credential_HMAC key"
)

type DestinationBigqueryDenormalizedLoadingMethodGCSStagingCredential struct {
	DestinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialHMACKey *DestinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialHMACKey

	Type DestinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialType
}

func CreateDestinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialDestinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialHMACKey(destinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialHMACKey DestinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialHMACKey) DestinationBigqueryDenormalizedLoadingMethodGCSStagingCredential {
	typ := DestinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialTypeDestinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialHMACKey

	return DestinationBigqueryDenormalizedLoadingMethodGCSStagingCredential{
		DestinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialHMACKey: &destinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialHMACKey,
		Type: typ,
	}
}

func (u *DestinationBigqueryDenormalizedLoadingMethodGCSStagingCredential) UnmarshalJSON(data []byte) error {
	var d *json.Decoder

	destinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialHMACKey := new(DestinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialHMACKey)
	d = json.NewDecoder(bytes.NewReader(data))
	d.DisallowUnknownFields()
	if err := d.Decode(&destinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialHMACKey); err == nil {
		u.DestinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialHMACKey = destinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialHMACKey
		u.Type = DestinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialTypeDestinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialHMACKey
		return nil
	}

	return errors.New("could not unmarshal into supported union types")
}

func (u DestinationBigqueryDenormalizedLoadingMethodGCSStagingCredential) MarshalJSON() ([]byte, error) {
	if u.DestinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialHMACKey != nil {
		return json.Marshal(u.DestinationBigqueryDenormalizedLoadingMethodGCSStagingCredentialHMACKey)
	}

	return nil, nil
}

// DestinationBigqueryDenormalizedLoadingMethodGCSStagingGCSTmpFilesAfterwardProcessingEnum - This upload method is supposed to temporary store records in GCS bucket. By this select you can chose if these records should be removed from GCS when migration has finished. The default "Delete all tmp files from GCS" value is used if not set explicitly.
type DestinationBigqueryDenormalizedLoadingMethodGCSStagingGCSTmpFilesAfterwardProcessingEnum string

const (
	DestinationBigqueryDenormalizedLoadingMethodGCSStagingGCSTmpFilesAfterwardProcessingEnumDeleteAllTmpFilesFromGcs DestinationBigqueryDenormalizedLoadingMethodGCSStagingGCSTmpFilesAfterwardProcessingEnum = "Delete all tmp files from GCS"
	DestinationBigqueryDenormalizedLoadingMethodGCSStagingGCSTmpFilesAfterwardProcessingEnumKeepAllTmpFilesInGcs     DestinationBigqueryDenormalizedLoadingMethodGCSStagingGCSTmpFilesAfterwardProcessingEnum = "Keep all tmp files in GCS"
)

func (e DestinationBigqueryDenormalizedLoadingMethodGCSStagingGCSTmpFilesAfterwardProcessingEnum) ToPointer() *DestinationBigqueryDenormalizedLoadingMethodGCSStagingGCSTmpFilesAfterwardProcessingEnum {
	return &e
}

func (e *DestinationBigqueryDenormalizedLoadingMethodGCSStagingGCSTmpFilesAfterwardProcessingEnum) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "Delete all tmp files from GCS":
		fallthrough
	case "Keep all tmp files in GCS":
		*e = DestinationBigqueryDenormalizedLoadingMethodGCSStagingGCSTmpFilesAfterwardProcessingEnum(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationBigqueryDenormalizedLoadingMethodGCSStagingGCSTmpFilesAfterwardProcessingEnum: %v", v)
	}
}

type DestinationBigqueryDenormalizedLoadingMethodGCSStagingMethodEnum string

const (
	DestinationBigqueryDenormalizedLoadingMethodGCSStagingMethodEnumGcsStaging DestinationBigqueryDenormalizedLoadingMethodGCSStagingMethodEnum = "GCS Staging"
)

func (e DestinationBigqueryDenormalizedLoadingMethodGCSStagingMethodEnum) ToPointer() *DestinationBigqueryDenormalizedLoadingMethodGCSStagingMethodEnum {
	return &e
}

func (e *DestinationBigqueryDenormalizedLoadingMethodGCSStagingMethodEnum) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "GCS Staging":
		*e = DestinationBigqueryDenormalizedLoadingMethodGCSStagingMethodEnum(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationBigqueryDenormalizedLoadingMethodGCSStagingMethodEnum: %v", v)
	}
}

// DestinationBigqueryDenormalizedLoadingMethodGCSStaging - Loading method used to send select the way data will be uploaded to BigQuery. <br/><b>Standard Inserts</b> - Direct uploading using SQL INSERT statements. This method is extremely inefficient and provided only for quick testing. In almost all cases, you should use staging. <br/><b>GCS Staging</b> - Writes large batches of records to a file, uploads the file to GCS, then uses <b>COPY INTO table</b> to upload the file. Recommended for most workloads for better speed and scalability. Read more about GCS Staging <a href="https://docs.airbyte.com/integrations/destinations/bigquery#gcs-staging">here</a>.
type DestinationBigqueryDenormalizedLoadingMethodGCSStaging struct {
	// An HMAC key is a type of credential and can be associated with a service account or a user account in Cloud Storage. Read more <a href="https://cloud.google.com/storage/docs/authentication/hmackeys">here</a>.
	Credential DestinationBigqueryDenormalizedLoadingMethodGCSStagingCredential `json:"credential"`
	// Number of file buffers allocated for writing data. Increasing this number is beneficial for connections using Change Data Capture (CDC) and up to the number of streams within a connection. Increasing the number of file buffers past the maximum number of streams has deteriorating effects
	FileBufferCount *int64 `json:"file_buffer_count,omitempty"`
	// The name of the GCS bucket. Read more <a href="https://cloud.google.com/storage/docs/naming-buckets">here</a>.
	GcsBucketName string `json:"gcs_bucket_name"`
	// Directory under the GCS bucket where data will be written. Read more <a href="https://cloud.google.com/storage/docs/locations">here</a>.
	GcsBucketPath string `json:"gcs_bucket_path"`
	// This upload method is supposed to temporary store records in GCS bucket. By this select you can chose if these records should be removed from GCS when migration has finished. The default "Delete all tmp files from GCS" value is used if not set explicitly.
	KeepFilesInGcsBucket *DestinationBigqueryDenormalizedLoadingMethodGCSStagingGCSTmpFilesAfterwardProcessingEnum `json:"keep_files_in_gcs-bucket,omitempty"`
	Method               DestinationBigqueryDenormalizedLoadingMethodGCSStagingMethodEnum                          `json:"method"`
}

type DestinationBigqueryDenormalizedLoadingMethodStandardInsertsMethodEnum string

const (
	DestinationBigqueryDenormalizedLoadingMethodStandardInsertsMethodEnumStandard DestinationBigqueryDenormalizedLoadingMethodStandardInsertsMethodEnum = "Standard"
)

func (e DestinationBigqueryDenormalizedLoadingMethodStandardInsertsMethodEnum) ToPointer() *DestinationBigqueryDenormalizedLoadingMethodStandardInsertsMethodEnum {
	return &e
}

func (e *DestinationBigqueryDenormalizedLoadingMethodStandardInsertsMethodEnum) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "Standard":
		*e = DestinationBigqueryDenormalizedLoadingMethodStandardInsertsMethodEnum(v)
		return nil
	default:
		return fmt.Errorf("invalid value for DestinationBigqueryDenormalizedLoadingMethodStandardInsertsMethodEnum: %v", v)
	}
}

// DestinationBigqueryDenormalizedLoadingMethodStandardInserts - Loading method used to send select the way data will be uploaded to BigQuery. <br/><b>Standard Inserts</b> - Direct uploading using SQL INSERT statements. This method is extremely inefficient and provided only for quick testing. In almost all cases, you should use staging. <br/><b>GCS Staging</b> - Writes large batches of records to a file, uploads the file to GCS, then uses <b>COPY INTO table</b> to upload the file. Recommended for most workloads for better speed and scalability. Read more about GCS Staging <a href="https://docs.airbyte.com/integrations/destinations/bigquery#gcs-staging">here</a>.
type DestinationBigqueryDenormalizedLoadingMethodStandardInserts struct {
	Method DestinationBigqueryDenormalizedLoadingMethodStandardInsertsMethodEnum `json:"method"`
}

type DestinationBigqueryDenormalizedLoadingMethodType string

const (
	DestinationBigqueryDenormalizedLoadingMethodTypeDestinationBigqueryDenormalizedLoadingMethodStandardInserts DestinationBigqueryDenormalizedLoadingMethodType = "destination-bigquery-denormalized_Loading Method_Standard Inserts"
	DestinationBigqueryDenormalizedLoadingMethodTypeDestinationBigqueryDenormalizedLoadingMethodGCSStaging      DestinationBigqueryDenormalizedLoadingMethodType = "destination-bigquery-denormalized_Loading Method_GCS Staging"
)

type DestinationBigqueryDenormalizedLoadingMethod struct {
	DestinationBigqueryDenormalizedLoadingMethodStandardInserts *DestinationBigqueryDenormalizedLoadingMethodStandardInserts
	DestinationBigqueryDenormalizedLoadingMethodGCSStaging      *DestinationBigqueryDenormalizedLoadingMethodGCSStaging

	Type DestinationBigqueryDenormalizedLoadingMethodType
}

func CreateDestinationBigqueryDenormalizedLoadingMethodDestinationBigqueryDenormalizedLoadingMethodStandardInserts(destinationBigqueryDenormalizedLoadingMethodStandardInserts DestinationBigqueryDenormalizedLoadingMethodStandardInserts) DestinationBigqueryDenormalizedLoadingMethod {
	typ := DestinationBigqueryDenormalizedLoadingMethodTypeDestinationBigqueryDenormalizedLoadingMethodStandardInserts

	return DestinationBigqueryDenormalizedLoadingMethod{
		DestinationBigqueryDenormalizedLoadingMethodStandardInserts: &destinationBigqueryDenormalizedLoadingMethodStandardInserts,
		Type: typ,
	}
}

func CreateDestinationBigqueryDenormalizedLoadingMethodDestinationBigqueryDenormalizedLoadingMethodGCSStaging(destinationBigqueryDenormalizedLoadingMethodGCSStaging DestinationBigqueryDenormalizedLoadingMethodGCSStaging) DestinationBigqueryDenormalizedLoadingMethod {
	typ := DestinationBigqueryDenormalizedLoadingMethodTypeDestinationBigqueryDenormalizedLoadingMethodGCSStaging

	return DestinationBigqueryDenormalizedLoadingMethod{
		DestinationBigqueryDenormalizedLoadingMethodGCSStaging: &destinationBigqueryDenormalizedLoadingMethodGCSStaging,
		Type: typ,
	}
}

func (u *DestinationBigqueryDenormalizedLoadingMethod) UnmarshalJSON(data []byte) error {
	var d *json.Decoder

	destinationBigqueryDenormalizedLoadingMethodStandardInserts := new(DestinationBigqueryDenormalizedLoadingMethodStandardInserts)
	d = json.NewDecoder(bytes.NewReader(data))
	d.DisallowUnknownFields()
	if err := d.Decode(&destinationBigqueryDenormalizedLoadingMethodStandardInserts); err == nil {
		u.DestinationBigqueryDenormalizedLoadingMethodStandardInserts = destinationBigqueryDenormalizedLoadingMethodStandardInserts
		u.Type = DestinationBigqueryDenormalizedLoadingMethodTypeDestinationBigqueryDenormalizedLoadingMethodStandardInserts
		return nil
	}

	destinationBigqueryDenormalizedLoadingMethodGCSStaging := new(DestinationBigqueryDenormalizedLoadingMethodGCSStaging)
	d = json.NewDecoder(bytes.NewReader(data))
	d.DisallowUnknownFields()
	if err := d.Decode(&destinationBigqueryDenormalizedLoadingMethodGCSStaging); err == nil {
		u.DestinationBigqueryDenormalizedLoadingMethodGCSStaging = destinationBigqueryDenormalizedLoadingMethodGCSStaging
		u.Type = DestinationBigqueryDenormalizedLoadingMethodTypeDestinationBigqueryDenormalizedLoadingMethodGCSStaging
		return nil
	}

	return errors.New("could not unmarshal into supported union types")
}

func (u DestinationBigqueryDenormalizedLoadingMethod) MarshalJSON() ([]byte, error) {
	if u.DestinationBigqueryDenormalizedLoadingMethodStandardInserts != nil {
		return json.Marshal(u.DestinationBigqueryDenormalizedLoadingMethodStandardInserts)
	}

	if u.DestinationBigqueryDenormalizedLoadingMethodGCSStaging != nil {
		return json.Marshal(u.DestinationBigqueryDenormalizedLoadingMethodGCSStaging)
	}

	return nil, nil
}

type DestinationBigqueryDenormalized struct {
	// Google BigQuery client's chunk (buffer) size (MIN=1, MAX = 15) for each table. The size that will be written by a single RPC. Written data will be buffered and only flushed upon reaching this size or closing the channel. The default 15MB value is used if not set explicitly. Read more <a href="https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.client.Client.html">here</a>.
	BigQueryClientBufferSizeMb *int64 `json:"big_query_client_buffer_size_mb,omitempty"`
	// The contents of the JSON service account key. Check out the <a href="https://docs.airbyte.com/integrations/destinations/bigquery#service-account-key">docs</a> if you need help generating this key. Default credentials will be used if this field is left empty.
	CredentialsJSON *string `json:"credentials_json,omitempty"`
	// The default BigQuery Dataset ID that tables are replicated to if the source does not specify a namespace. Read more <a href="https://cloud.google.com/bigquery/docs/datasets#create-dataset">here</a>.
	DatasetID string `json:"dataset_id"`
	// The location of the dataset. Warning: Changes made after creation will not be applied. The default "US" value is used if not set explicitly. Read more <a href="https://cloud.google.com/bigquery/docs/locations">here</a>.
	DatasetLocation *DestinationBigqueryDenormalizedDatasetLocationEnum     `json:"dataset_location,omitempty"`
	DestinationType DestinationBigqueryDenormalizedBigqueryDenormalizedEnum `json:"destinationType"`
	// Loading method used to send select the way data will be uploaded to BigQuery. <br/><b>Standard Inserts</b> - Direct uploading using SQL INSERT statements. This method is extremely inefficient and provided only for quick testing. In almost all cases, you should use staging. <br/><b>GCS Staging</b> - Writes large batches of records to a file, uploads the file to GCS, then uses <b>COPY INTO table</b> to upload the file. Recommended for most workloads for better speed and scalability. Read more about GCS Staging <a href="https://docs.airbyte.com/integrations/destinations/bigquery#gcs-staging">here</a>.
	LoadingMethod *DestinationBigqueryDenormalizedLoadingMethod `json:"loading_method,omitempty"`
	// The GCP project ID for the project containing the target BigQuery dataset. Read more <a href="https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects">here</a>.
	ProjectID string `json:"project_id"`
}
