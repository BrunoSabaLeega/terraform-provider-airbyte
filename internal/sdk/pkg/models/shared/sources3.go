// Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT.

package shared

import (
	"bytes"
	"encoding/json"
	"errors"
	"fmt"
)

type SourceS3FileFormatJsonlFiletypeEnum string

const (
	SourceS3FileFormatJsonlFiletypeEnumJsonl SourceS3FileFormatJsonlFiletypeEnum = "jsonl"
)

func (e SourceS3FileFormatJsonlFiletypeEnum) ToPointer() *SourceS3FileFormatJsonlFiletypeEnum {
	return &e
}

func (e *SourceS3FileFormatJsonlFiletypeEnum) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "jsonl":
		*e = SourceS3FileFormatJsonlFiletypeEnum(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SourceS3FileFormatJsonlFiletypeEnum: %v", v)
	}
}

// SourceS3FileFormatJsonlUnexpectedFieldBehaviorEnum - How JSON fields outside of explicit_schema (if given) are treated. Check <a href="https://arrow.apache.org/docs/python/generated/pyarrow.json.ParseOptions.html" target="_blank">PyArrow documentation</a> for details
type SourceS3FileFormatJsonlUnexpectedFieldBehaviorEnum string

const (
	SourceS3FileFormatJsonlUnexpectedFieldBehaviorEnumIgnore SourceS3FileFormatJsonlUnexpectedFieldBehaviorEnum = "ignore"
	SourceS3FileFormatJsonlUnexpectedFieldBehaviorEnumInfer  SourceS3FileFormatJsonlUnexpectedFieldBehaviorEnum = "infer"
	SourceS3FileFormatJsonlUnexpectedFieldBehaviorEnumError  SourceS3FileFormatJsonlUnexpectedFieldBehaviorEnum = "error"
)

func (e SourceS3FileFormatJsonlUnexpectedFieldBehaviorEnum) ToPointer() *SourceS3FileFormatJsonlUnexpectedFieldBehaviorEnum {
	return &e
}

func (e *SourceS3FileFormatJsonlUnexpectedFieldBehaviorEnum) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "ignore":
		fallthrough
	case "infer":
		fallthrough
	case "error":
		*e = SourceS3FileFormatJsonlUnexpectedFieldBehaviorEnum(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SourceS3FileFormatJsonlUnexpectedFieldBehaviorEnum: %v", v)
	}
}

// SourceS3FileFormatJsonl - This connector uses <a href="https://arrow.apache.org/docs/python/json.html" target="_blank">PyArrow</a> for JSON Lines (jsonl) file parsing.
type SourceS3FileFormatJsonl struct {
	// The chunk size in bytes to process at a time in memory from each file. If your data is particularly wide and failing during schema detection, increasing this should solve it. Beware of raising this too high as you could hit OOM errors.
	BlockSize *int64                               `json:"block_size,omitempty"`
	Filetype  *SourceS3FileFormatJsonlFiletypeEnum `json:"filetype,omitempty"`
	// Whether newline characters are allowed in JSON values. Turning this on may affect performance. Leave blank to default to False.
	NewlinesInValues *bool `json:"newlines_in_values,omitempty"`
	// How JSON fields outside of explicit_schema (if given) are treated. Check <a href="https://arrow.apache.org/docs/python/generated/pyarrow.json.ParseOptions.html" target="_blank">PyArrow documentation</a> for details
	UnexpectedFieldBehavior *SourceS3FileFormatJsonlUnexpectedFieldBehaviorEnum `json:"unexpected_field_behavior,omitempty"`
}

type SourceS3FileFormatAvroFiletypeEnum string

const (
	SourceS3FileFormatAvroFiletypeEnumAvro SourceS3FileFormatAvroFiletypeEnum = "avro"
)

func (e SourceS3FileFormatAvroFiletypeEnum) ToPointer() *SourceS3FileFormatAvroFiletypeEnum {
	return &e
}

func (e *SourceS3FileFormatAvroFiletypeEnum) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "avro":
		*e = SourceS3FileFormatAvroFiletypeEnum(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SourceS3FileFormatAvroFiletypeEnum: %v", v)
	}
}

// SourceS3FileFormatAvro - This connector utilises <a href="https://fastavro.readthedocs.io/en/latest/" target="_blank">fastavro</a> for Avro parsing.
type SourceS3FileFormatAvro struct {
	Filetype *SourceS3FileFormatAvroFiletypeEnum `json:"filetype,omitempty"`
}

type SourceS3FileFormatParquetFiletypeEnum string

const (
	SourceS3FileFormatParquetFiletypeEnumParquet SourceS3FileFormatParquetFiletypeEnum = "parquet"
)

func (e SourceS3FileFormatParquetFiletypeEnum) ToPointer() *SourceS3FileFormatParquetFiletypeEnum {
	return &e
}

func (e *SourceS3FileFormatParquetFiletypeEnum) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "parquet":
		*e = SourceS3FileFormatParquetFiletypeEnum(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SourceS3FileFormatParquetFiletypeEnum: %v", v)
	}
}

// SourceS3FileFormatParquet - This connector utilises <a href="https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetFile.html" target="_blank">PyArrow (Apache Arrow)</a> for Parquet parsing.
type SourceS3FileFormatParquet struct {
	// Maximum number of records per batch read from the input files. Batches may be smaller if there arenâ€™t enough rows in the file. This option can help avoid out-of-memory errors if your data is particularly wide.
	BatchSize *int64 `json:"batch_size,omitempty"`
	// Perform read buffering when deserializing individual column chunks. By default every group column will be loaded fully to memory. This option can help avoid out-of-memory errors if your data is particularly wide.
	BufferSize *int64 `json:"buffer_size,omitempty"`
	// If you only want to sync a subset of the columns from the file(s), add the columns you want here as a comma-delimited list. Leave it empty to sync all columns.
	Columns  []string                               `json:"columns,omitempty"`
	Filetype *SourceS3FileFormatParquetFiletypeEnum `json:"filetype,omitempty"`
}

type SourceS3FileFormatCSVFiletypeEnum string

const (
	SourceS3FileFormatCSVFiletypeEnumCsv SourceS3FileFormatCSVFiletypeEnum = "csv"
)

func (e SourceS3FileFormatCSVFiletypeEnum) ToPointer() *SourceS3FileFormatCSVFiletypeEnum {
	return &e
}

func (e *SourceS3FileFormatCSVFiletypeEnum) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "csv":
		*e = SourceS3FileFormatCSVFiletypeEnum(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SourceS3FileFormatCSVFiletypeEnum: %v", v)
	}
}

// SourceS3FileFormatCSV - This connector utilises <a href="https: // arrow.apache.org/docs/python/generated/pyarrow.csv.open_csv.html" target="_blank">PyArrow (Apache Arrow)</a> for CSV parsing.
type SourceS3FileFormatCSV struct {
	// Optionally add a valid JSON string here to provide additional options to the csv reader. Mappings must correspond to options <a href="https://arrow.apache.org/docs/python/generated/pyarrow.csv.ConvertOptions.html#pyarrow.csv.ConvertOptions" target="_blank">detailed here</a>. 'column_types' is used internally to handle schema so overriding that would likely cause problems.
	AdditionalReaderOptions *string `json:"additional_reader_options,omitempty"`
	// Optionally add a valid JSON string here to provide additional <a href="https://arrow.apache.org/docs/python/generated/pyarrow.csv.ReadOptions.html#pyarrow.csv.ReadOptions" target="_blank">Pyarrow ReadOptions</a>. Specify 'column_names' here if your CSV doesn't have header, or if you want to use custom column names. 'block_size' and 'encoding' are already used above, specify them again here will override the values above.
	AdvancedOptions *string `json:"advanced_options,omitempty"`
	// The chunk size in bytes to process at a time in memory from each file. If your data is particularly wide and failing during schema detection, increasing this should solve it. Beware of raising this too high as you could hit OOM errors.
	BlockSize *int64 `json:"block_size,omitempty"`
	// The character delimiting individual cells in the CSV data. This may only be a 1-character string. For tab-delimited data enter '	'.
	Delimiter *string `json:"delimiter,omitempty"`
	// Whether two quotes in a quoted CSV value denote a single quote in the data.
	DoubleQuote *bool `json:"double_quote,omitempty"`
	// The character encoding of the CSV data. Leave blank to default to <strong>UTF8</strong>. See <a href="https://docs.python.org/3/library/codecs.html#standard-encodings" target="_blank">list of python encodings</a> for allowable options.
	Encoding *string `json:"encoding,omitempty"`
	// The character used for escaping special characters. To disallow escaping, leave this field blank.
	EscapeChar *string                            `json:"escape_char,omitempty"`
	Filetype   *SourceS3FileFormatCSVFiletypeEnum `json:"filetype,omitempty"`
	// Configures whether a schema for the source should be inferred from the current data or not. If set to false and a custom schema is set, then the manually enforced schema is used. If a schema is not manually set, and this is set to false, then all fields will be read as strings
	InferDatatypes *bool `json:"infer_datatypes,omitempty"`
	// Whether newline characters are allowed in CSV values. Turning this on may affect performance. Leave blank to default to False.
	NewlinesInValues *bool `json:"newlines_in_values,omitempty"`
	// The character used for quoting CSV values. To disallow quoting, make this field blank.
	QuoteChar *string `json:"quote_char,omitempty"`
}

type SourceS3FileFormatType string

const (
	SourceS3FileFormatTypeSourceS3FileFormatCSV     SourceS3FileFormatType = "source-s3_File Format_CSV"
	SourceS3FileFormatTypeSourceS3FileFormatParquet SourceS3FileFormatType = "source-s3_File Format_Parquet"
	SourceS3FileFormatTypeSourceS3FileFormatAvro    SourceS3FileFormatType = "source-s3_File Format_Avro"
	SourceS3FileFormatTypeSourceS3FileFormatJsonl   SourceS3FileFormatType = "source-s3_File Format_Jsonl"
)

type SourceS3FileFormat struct {
	SourceS3FileFormatCSV     *SourceS3FileFormatCSV
	SourceS3FileFormatParquet *SourceS3FileFormatParquet
	SourceS3FileFormatAvro    *SourceS3FileFormatAvro
	SourceS3FileFormatJsonl   *SourceS3FileFormatJsonl

	Type SourceS3FileFormatType
}

func CreateSourceS3FileFormatSourceS3FileFormatCSV(sourceS3FileFormatCSV SourceS3FileFormatCSV) SourceS3FileFormat {
	typ := SourceS3FileFormatTypeSourceS3FileFormatCSV

	return SourceS3FileFormat{
		SourceS3FileFormatCSV: &sourceS3FileFormatCSV,
		Type:                  typ,
	}
}

func CreateSourceS3FileFormatSourceS3FileFormatParquet(sourceS3FileFormatParquet SourceS3FileFormatParquet) SourceS3FileFormat {
	typ := SourceS3FileFormatTypeSourceS3FileFormatParquet

	return SourceS3FileFormat{
		SourceS3FileFormatParquet: &sourceS3FileFormatParquet,
		Type:                      typ,
	}
}

func CreateSourceS3FileFormatSourceS3FileFormatAvro(sourceS3FileFormatAvro SourceS3FileFormatAvro) SourceS3FileFormat {
	typ := SourceS3FileFormatTypeSourceS3FileFormatAvro

	return SourceS3FileFormat{
		SourceS3FileFormatAvro: &sourceS3FileFormatAvro,
		Type:                   typ,
	}
}

func CreateSourceS3FileFormatSourceS3FileFormatJsonl(sourceS3FileFormatJsonl SourceS3FileFormatJsonl) SourceS3FileFormat {
	typ := SourceS3FileFormatTypeSourceS3FileFormatJsonl

	return SourceS3FileFormat{
		SourceS3FileFormatJsonl: &sourceS3FileFormatJsonl,
		Type:                    typ,
	}
}

func (u *SourceS3FileFormat) UnmarshalJSON(data []byte) error {
	var d *json.Decoder

	sourceS3FileFormatCSV := new(SourceS3FileFormatCSV)
	d = json.NewDecoder(bytes.NewReader(data))
	d.DisallowUnknownFields()
	if err := d.Decode(&sourceS3FileFormatCSV); err == nil {
		u.SourceS3FileFormatCSV = sourceS3FileFormatCSV
		u.Type = SourceS3FileFormatTypeSourceS3FileFormatCSV
		return nil
	}

	sourceS3FileFormatParquet := new(SourceS3FileFormatParquet)
	d = json.NewDecoder(bytes.NewReader(data))
	d.DisallowUnknownFields()
	if err := d.Decode(&sourceS3FileFormatParquet); err == nil {
		u.SourceS3FileFormatParquet = sourceS3FileFormatParquet
		u.Type = SourceS3FileFormatTypeSourceS3FileFormatParquet
		return nil
	}

	sourceS3FileFormatAvro := new(SourceS3FileFormatAvro)
	d = json.NewDecoder(bytes.NewReader(data))
	d.DisallowUnknownFields()
	if err := d.Decode(&sourceS3FileFormatAvro); err == nil {
		u.SourceS3FileFormatAvro = sourceS3FileFormatAvro
		u.Type = SourceS3FileFormatTypeSourceS3FileFormatAvro
		return nil
	}

	sourceS3FileFormatJsonl := new(SourceS3FileFormatJsonl)
	d = json.NewDecoder(bytes.NewReader(data))
	d.DisallowUnknownFields()
	if err := d.Decode(&sourceS3FileFormatJsonl); err == nil {
		u.SourceS3FileFormatJsonl = sourceS3FileFormatJsonl
		u.Type = SourceS3FileFormatTypeSourceS3FileFormatJsonl
		return nil
	}

	return errors.New("could not unmarshal into supported union types")
}

func (u SourceS3FileFormat) MarshalJSON() ([]byte, error) {
	if u.SourceS3FileFormatCSV != nil {
		return json.Marshal(u.SourceS3FileFormatCSV)
	}

	if u.SourceS3FileFormatParquet != nil {
		return json.Marshal(u.SourceS3FileFormatParquet)
	}

	if u.SourceS3FileFormatAvro != nil {
		return json.Marshal(u.SourceS3FileFormatAvro)
	}

	if u.SourceS3FileFormatJsonl != nil {
		return json.Marshal(u.SourceS3FileFormatJsonl)
	}

	return nil, nil
}

// SourceS3S3AmazonWebServices - Use this to load files from S3 or S3-compatible services
type SourceS3S3AmazonWebServices struct {
	// In order to access private Buckets stored on AWS S3, this connector requires credentials with the proper permissions. If accessing publicly available data, this field is not necessary.
	AwsAccessKeyID *string `json:"aws_access_key_id,omitempty"`
	// In order to access private Buckets stored on AWS S3, this connector requires credentials with the proper permissions. If accessing publicly available data, this field is not necessary.
	AwsSecretAccessKey *string `json:"aws_secret_access_key,omitempty"`
	// Name of the S3 bucket where the file(s) exist.
	Bucket string `json:"bucket"`
	// Endpoint to an S3 compatible service. Leave empty to use AWS.
	Endpoint *string `json:"endpoint,omitempty"`
	// By providing a path-like prefix (e.g. myFolder/thisTable/) under which all the relevant files sit, we can optimize finding these in S3. This is optional but recommended if your bucket contains many folders/files which you don't need to replicate.
	PathPrefix *string `json:"path_prefix,omitempty"`
}

type SourceS3S3Enum string

const (
	SourceS3S3EnumS3 SourceS3S3Enum = "s3"
)

func (e SourceS3S3Enum) ToPointer() *SourceS3S3Enum {
	return &e
}

func (e *SourceS3S3Enum) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "s3":
		*e = SourceS3S3Enum(v)
		return nil
	default:
		return fmt.Errorf("invalid value for SourceS3S3Enum: %v", v)
	}
}

// SourceS3 - The values required to configure the source.
type SourceS3 struct {
	// The name of the stream you would like this source to output. Can contain letters, numbers, or underscores.
	Dataset string `json:"dataset"`
	// The format of the files you'd like to replicate
	Format *SourceS3FileFormat `json:"format,omitempty"`
	// A regular expression which tells the connector which files to replicate. All files which match this pattern will be replicated. Use | to separate multiple patterns. See <a href="https://facelessuser.github.io/wcmatch/glob/" target="_blank">this page</a> to understand pattern syntax (GLOBSTAR and SPLIT flags are enabled). Use pattern <strong>**</strong> to pick up all files.
	PathPattern string `json:"path_pattern"`
	// Use this to load files from S3 or S3-compatible services
	Provider SourceS3S3AmazonWebServices `json:"provider"`
	// Optionally provide a schema to enforce, as a valid JSON string. Ensure this is a mapping of <strong>{ "column" : "type" }</strong>, where types are valid <a href="https://json-schema.org/understanding-json-schema/reference/type.html" target="_blank">JSON Schema datatypes</a>. Leave as {} to auto-infer the schema.
	Schema     *string        `json:"schema,omitempty"`
	SourceType SourceS3S3Enum `json:"sourceType"`
}
